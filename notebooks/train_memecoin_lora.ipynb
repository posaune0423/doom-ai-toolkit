{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Toolkit by Ostris\n",
    "## Memecoin Multi-Concept LoRA Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "if os.path.exists('doom-ai-toolkit'):\n",
    "    subprocess.run(['git', '-C', 'doom-ai-toolkit', 'pull', 'origin', 'main'], check=True)\n",
    "    print('Repository updated successfully')\n",
    "else:\n",
    "    subprocess.run(['git', 'clone', 'https://github.com/posaune0423/doom-ai-toolkit.git'], check=True)\n",
    "    print('Repository cloned successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put your image dataset in the `dataset/` folder at the repo root. This repo already includes the memecoin data in that location, so no extra setup is required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Install jedi first (required by ipython)\n",
    "print(\"Installing jedi...\")\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"jedi>=0.16\"], check=True)\n",
    "\n",
    "# Install numpy compatible with easy-dwpose (numpy<2.0.0 required)\n",
    "# Note: Colab environment may have packages requiring numpy>=2.0, but they are not part of this project\n",
    "print(\"Installing numpy...\")\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"--force-reinstall\", \"numpy>=1.26.4,<2.0.0\"], check=True)\n",
    "\n",
    "# Reinstall PyTorch to ensure binary compatibility with the numpy version we just installed\n",
    "# This is critical to avoid \"numpy.dtype size changed\" errors\n",
    "print(\"Checking PyTorch installation...\")\n",
    "result = subprocess.run([sys.executable, \"-m\", \"pip\", \"show\", \"torch\"], capture_output=True, text=True)\n",
    "if result.returncode == 0:\n",
    "    print(\"PyTorch is already installed, reinstalling for numpy compatibility...\")\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"--force-reinstall\",\n",
    "                    \"torch\", \"torchvision\", \"torchaudio\"], check=True)\n",
    "else:\n",
    "    print(\"PyTorch not found, installing...\")\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\",\n",
    "                    \"torch\", \"torchvision\", \"torchaudio\"], check=True)\n",
    "\n",
    "# Change to doom-ai-toolkit directory and update submodules\n",
    "print(\"Updating git submodules...\")\n",
    "os.chdir('doom-ai-toolkit')\n",
    "subprocess.run([\"git\", \"submodule\", \"update\", \"--init\", \"--recursive\"], check=True)\n",
    "\n",
    "# Install huggingface_hub with correct version first (before transformers)\n",
    "# This fixes the ImportError: huggingface-hub>=0.30.0,<1.0 is required\n",
    "print(\"Installing huggingface_hub with correct version...\")\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"--force-reinstall\",\n",
    "                \"huggingface_hub>=0.30.0,<1.0\"], check=True)\n",
    "\n",
    "# Install dependencies from requirements.txt (but skip diffusers and easy-dwpose for now)\n",
    "print(\"Installing dependencies from requirements.txt...\")\n",
    "# Read requirements.txt and filter out diffusers and easy-dwpose\n",
    "with open('requirements.txt', 'r') as f:\n",
    "    requirements = f.readlines()\n",
    "\n",
    "filtered_requirements = []\n",
    "for req in requirements:\n",
    "    req = req.strip()\n",
    "    if req and not req.startswith('#') and 'diffusers' not in req and 'easy_dwpose' not in req:\n",
    "        filtered_requirements.append(req)\n",
    "\n",
    "# Install filtered requirements\n",
    "if filtered_requirements:\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\"] + filtered_requirements, check=True)\n",
    "\n",
    "# Reinstall diffusers to ensure binary compatibility with numpy\n",
    "# This fixes the \"numpy.dtype size changed\" error when importing diffusers.schedulers\n",
    "# See: https://github.com/ostris/ai-toolkit/issues/267\n",
    "print(\"Reinstalling diffusers for numpy compatibility...\")\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"--force-reinstall\",\n",
    "                \"git+https://github.com/huggingface/diffusers@1448b035859dd57bbb565239dcdd79a025a85422\"], check=True)\n",
    "\n",
    "# Install easy-dwpose separately with --no-deps to avoid numpy version conflicts\n",
    "# easy-dwpose requires numpy<2.0.0, which we've already installed above\n",
    "print(\"Installing easy-dwpose...\")\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"--no-deps\",\n",
    "                \"git+https://github.com/jaretburkett/easy_dwpose.git\"], check=True)\n",
    "\n",
    "print(\"All dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model License\n",
    "FLUX.1-schnell is Apache 2.0 and does not require a gated Hugging Face token, but you still need to agree to the model terms on Hugging Face and keep a token handy if you plan to access private models. Use the next cell to set `HF_TOKEN` when needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "SECRET_NAME = \"HF_TOKEN\"\n",
    "\n",
    "from google.colab import userdata  # type: ignore\n",
    "\n",
    "hf_token = userdata.get(SECRET_NAME)\n",
    "if not hf_token:\n",
    "    raise RuntimeError(\n",
    "        f\"Failed to retrieve required Hugging Face token from Colab secret '{SECRET_NAME}'.\"\n",
    "    )\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = hf_token\n",
    "print(f\"HF_TOKEN environment variable has been set from Colab secret '{SECRET_NAME}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('/content/doom-ai-toolkit')\n",
    "from toolkit.job import run_job\n",
    "import yaml\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This is your config. It is documented pretty well. The configuration is loaded from a YAML file. This will run as is without modification, but feel free to edit as you want.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "config_path = Path('/content/doom-ai-toolkit/config/train_memecoin_multi_lora.yaml')\n",
    "job_config = yaml.safe_load(config_path.read_text())\n",
    "print('Configuration loaded')\n",
    "print(f\"Training: {job_config['config']['name']}\")\n",
    "print(f\"Datasets: {len(job_config['config']['process'][0]['datasets'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run it\n",
    "\n",
    "Below does all the magic. Check your folders to the left. Items will be in output/memecoin_multi_lora_v1. In the samples folder, there are periodic samples. This doesn't work great with colab. They will be in /content/doom-ai-toolkit/output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_job(job_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done\n",
    "\n",
    "Check your output dir and get your LoRA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "import os\n",
    "\n",
    "samples_dir = '/content/doom-ai-toolkit/output/memecoin_multi_lora_v1/samples'\n",
    "if os.path.exists(samples_dir):\n",
    "    images = sorted([f for f in os.listdir(samples_dir) if f.endswith('.png')])\n",
    "    for img in images[-6:]:\n",
    "        display(Image(filename=os.path.join(samples_dir, img)))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
