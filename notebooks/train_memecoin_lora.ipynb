{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Toolkit by Ostris\n",
    "## Memecoin Multi-Concept LoRA Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "if os.path.exists('doom-ai-toolkit'):\n",
    "    subprocess.run(['git', '-C', 'doom-ai-toolkit', 'pull', 'origin', 'main'], check=True)\n",
    "    print('Repository updated successfully')\n",
    "else:\n",
    "    subprocess.run(['git', 'clone', 'https://github.com/posaune0423/doom-ai-toolkit.git'], check=True)\n",
    "    print('Repository cloned successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses the dataset from the git repository. The memecoin dataset is already included in the `dataset/` folder at the repo root, so no extra setup is required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd doom-ai-toolkit && git submodule update --init --recursive && pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "!pip install --quiet --force-reinstall --no-deps numpy==1.26.3\n",
    "\n",
    "os.kill(os.getpid(), 9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model License\n",
    "FLUX.1-schnell is Apache 2.0 and does not require a gated Hugging Face token, but you still need to agree to the model terms on Hugging Face and keep a token handy if you plan to access private models. Use the next cell to set `HF_TOKEN` when needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "SECRET_NAME = \"HF_TOKEN\"\n",
    "\n",
    "from google.colab import userdata  # type: ignore\n",
    "\n",
    "hf_token = userdata.get(SECRET_NAME)\n",
    "if not hf_token:\n",
    "    raise RuntimeError(\n",
    "        f\"Failed to retrieve required Hugging Face token from Colab secret '{SECRET_NAME}'.\"\n",
    "    )\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = hf_token\n",
    "print(f\"HF_TOKEN environment variable has been set from Colab secret '{SECRET_NAME}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('/content/doom-ai-toolkit')\n",
    "from toolkit.job import run_job\n",
    "import yaml\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This is your config. It is documented pretty well. The configuration is loaded from a YAML file. This will run as is without modification, but feel free to edit as you want.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "config_path = Path('/content/doom-ai-toolkit/config/train_memecoin_pack_lora.yaml')\n",
    "job_to_run = yaml.safe_load(config_path.read_text())\n",
    "print('Configuration loaded')\n",
    "print(f\"Training: {job_to_run['config']['name']}\")\n",
    "print(f\"Datasets: {len(job_to_run['config']['process'][0]['datasets'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run it\n",
    "\n",
    "Below does all the magic. Check your folders to the left. Items will be in output/memecoin_multi_lora_v1. In the samples folder, there are periodic samples. This doesn't work great with colab. They will be in /content/doom-ai-toolkit/output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_job(job_to_run)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done\n",
    "\n",
    "Check your output dir and get your LoRA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# ベースのoutputディレクトリ（環境に合わせて必要なら変更）\n",
    "base_output_dir = Path('/content/doom-ai-toolkit/output')\n",
    "\n",
    "# output配下の「*/samples」ディレクトリを全て集めて、更新日時が新しいものを選ぶ\n",
    "sample_dirs = [p for p in base_output_dir.glob('*/samples') if p.is_dir()]\n",
    "if sample_dirs:\n",
    "    samples_dir = sorted(sample_dirs, key=lambda p: p.stat().st_mtime)[-1]\n",
    "    print(f'Using samples_dir: {samples_dir}')\n",
    "\n",
    "    images = sorted([f for f in os.listdir(samples_dir) if f.endswith('.png')])\n",
    "    for img in images[-6:]:\n",
    "        display(Image(filename=os.path.join(samples_dir, img)))\n",
    "else:\n",
    "    print(f'No samples directory found under {base_output_dir}')\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
