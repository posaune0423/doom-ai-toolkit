{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Toolkit by Ostris\n",
    "## Memecoin Multi-Concept LoRA Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/posaune0423/doom-ai-toolkit.git\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put your image dataset in the `/content/dataset` folder. This repo already includes `content/dataset` with the memecoin data, so no extra setup is required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd doom-ai-toolkit && git submodule update --init --recursive && pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model License\n",
    "FLUX.1-schnell is Apache 2.0 and does not require a gated Hugging Face token, but you still need to agree to the model terms on Hugging Face and keep a token handy if you plan to access private models. Use the next cell to set `HF_TOKEN` when needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "hf_token = getpass.getpass('Enter your HF access token (press enter to skip): ')\n",
    "\n",
    "if hf_token:\n",
    "    os.environ['HF_TOKEN'] = hf_token\n",
    "    print('HF_TOKEN environment variable has been set.')\n",
    "else:\n",
    "    print('No HF token provided; proceeding without setting HF_TOKEN.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('/content/doom-ai-toolkit')\n",
    "from toolkit.job import run_job\n",
    "import yaml\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This is your config. It is documented pretty well. The configuration is loaded from a YAML file. This will run as is without modification, but feel free to edit as you want.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd doom-ai-toolkit\n",
    "\n",
    "with open('config/train_memecoin_multi_lora.yaml', 'r') as f:\n",
    "    job_config = yaml.safe_load(f)\n",
    "print('Configuration loaded')\n",
    "print(f\"Training: {job_config['config']['name']}\")\n",
    "print(f\"Datasets: {len(job_config['config']['process'][0]['datasets'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run it\n",
    "\n",
    "Below does all the magic. Check your folders to the left. Items will be in output/memecoin_multi_lora_v1. In the samples folder, there are periodic samples. This doesn't work great with colab. They will be in /content/doom-ai-toolkit/output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_job(job_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done\n",
    "\n",
    "Check your output dir and get your LoRA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "import os\n",
    "\n",
    "samples_dir = 'output/memecoin_multi_lora_v1/samples'\n",
    "if os.path.exists(samples_dir):\n",
    "    images = sorted([f for f in os.listdir(samples_dir) if f.endswith('.png')])\n",
    "    for img in images[-6:]:\n",
    "        display(Image(filename=os.path.join(samples_dir, img)))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
